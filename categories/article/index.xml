<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Article on Yesphet</title>
    <link>https://yesphet.github.io/categories/article/</link>
    <description>Recent content in Article on Yesphet</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Feb 2019 15:05:35 +0800</lastBuildDate>
    
	<atom:link href="https://yesphet.github.io/categories/article/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Serverless and Faas</title>
      <link>https://yesphet.github.io/posts/faas-serverless/</link>
      <pubDate>Thu, 21 Feb 2019 15:05:35 +0800</pubDate>
      
      <guid>https://yesphet.github.io/posts/faas-serverless/</guid>
      <description>一、Serverless/Faas 什么是Serverless？在云时代，简单的说就是服务器对开发者是透明的 。当然并不仅是这么简单，只是我认为在还不能完全做到服务器对开发者透明的时候，探讨Severless更多的含义是没有太大意义的。因为无论是什么技术的出现，其目的都是为了提高生产效率。在计算机领域，提高生产效率的方式有很多种，降低程序员的心智负担，降低开发门槛是其中非常重要的一种，毕竟现在还是以人为本的时代。而Severless很重要的一个能力便是极大极大地减轻开发人员的负担，更简单的说便是极大地减少每一个项目的代码量。
作为一个菜鸡，我没有资格探讨什么计算机的前世今生。但就像汇编语言让机器语言变成透明，高级语言让汇编语言变成透明一样，Serverless可以让服务器逐渐变得透明。服务端开发人员不再必须明白操作系统、计算机网络等底层内容就能够很好地开发维护和使用一个服务，就像现在的程序员已经不再必须明白电路信号就能开始写代码一样。
这是我认知的Serverless，但我一直还觉得Serverless远不只于此。
那什么是Function as a Service（Faas）呢，我认同Faas是Serverless的一种实现。通过Faas，开发人员只需要专注于业务代码，不需要去关心服务器环境之类，自然服务器对开发者就是透明的。当然Serverless还有许多其他的实现，例如云存储（AWS的S3/阿里云的OSS）就是Serverless在存储系统上的一种实现。
有了Faas，程序员开发上线一个Hello World服务，就真的只要在代码里写下Hello World，就有了Hello World这个服务了。不需要关心服务器在哪里，怎么配置搭建环境，怎么打包发布。甚至当你写了Hello World这些代码发布成服务后，你不需要任何操作就可以直接获得这个服务的监控，比如多少访问QPS、成功率多少、响应时间多少，当成功率低于多少时你便会收到报警。这些所有的配套设施Faas都帮你做了，这就是Faas的魅力，是非常切实地在提升生产效率。
在物理机时代，只有优秀的程序员，可以掌握从设计到架构，从开发到上线，可以精准地定位解决问题，不单是程序的，还包括各个层面导致的问题。而通过Faas，不那么优秀的程序员，也可以做到和优秀程序员一样的事了。这是一本秘籍，一本化繁为简的秘籍。不需要那么多的架构设计，我只需要关注并设计好这个小小的Function，没有那么多问题需要排查了，就这几十几百行的代码能有多少问题呢？Faas平台有缺陷，有漏洞？那是Faas维护者的事，反馈就完事了。就像现在有多少程序员能真的帮忙解决修复操作系统的缺陷呢？而优秀的程序员，也不再需要被业务缠身，轻松完成开发上线，从而有更多的精力投身更多的建设。
当然，现在的Faas已经远不止我说的这些。它还有许多优秀的特性，我们继续分析。
二、Faas的特性 现在，Faas已经从边缘走向了中心，越来越频繁地出现在技术人员的视线中。其具有最明显的优势是：
 按需计算 简化开发 事件驱动 小巧灵活  2.1 按需计算 Faas是真正的按需计算，这些计算任务需要多少资源，就给你多少资源。
在离线任务的场景下，这很好实现，因为一开始我就知道有多少计算任务，需要多少计算资源。但在在线任务场景下呢？谁也不知道下一个流量峰刺什么时候到来。在物理机时代，大家只能按照经验准备足够的物理机冗余。在云主机时代，也需要提前扩容VPS，这些不得不的冗余虽然一定程度保障了服务的安全，但同时也造成了资源的浪费，这是资源等待请求的模式。而Faas，通过毫秒级别的冷启动时间，并不需要提前启动好资源等待请求。而是可以等请求到来的时候，让请求等待资源启动，然后进行计算，计算完成后释放计算资源。这是请求等待资源的模式，是完全按需的。这个等待时间是极短的。可能有些业务场景容忍不了这额外带来的毫秒级别的等待，但faas依然可以通过极少的资源冗余和弹性伸缩去解决这些等待时间，让请求几乎无感知等待耗时。
弹性伸缩便是通过各项指标，来预测资源冗余是否充足或过多，从而提前自动扩容缩容。这些指标简单可以是CPU使用率、Load值、内存使用率等资源指标，但这样的指标其实已经稍微偏离了业务，并不能达到非常准确的预测。更高级地可以通过业务QPS的增长率等贴近业务的指标来作为判断依据，更为准确，也更专注于业务。
通过这种真正的按需计算，也可以真正达到cost-efficient，high utilization的目标。
2.2 简化开发 如第一部分说的，在Faas下，开发人员只需要专注在业务代码中，编程模型得到了极大的简化。除此之外，还将很多事情让渡给了云。例如2.1中说的弹性伸缩，其实在物理机、云主机模式下也可以实现，但在这些模式下，不够干净的隔离，只会给开发人员带来更多的心智负担。Faas还为开发人员完成许多想做但又可能没有精力做的事情，例如完善的监控于报警系统等等。
2.3 事件驱动 现在很多Faas实现的协议都是遵循事件驱动的。也就是除了普通的Http请求、消费消息队列，还有许多的方式去触发计算，例如文件上传到云存储，可以自动触发内容识别、审核、分类的Function。这也简化许多业务的流程。
2.4 小巧灵活 在Faas下，期望的是能做到足够的解耦，每个Function都足够的小，因此能运行在许许多多的计算资源上。通常情况下我们认知的计算资源就是服务器，但其实除了专用服务器，还有很多很多计算资源没有被好好利用。例如CDN的边缘节点、IoT中许多空闲的智能设备。Faas为利用这些资源带来了可能，因为一定有足够小的Function，小到可以用这些轻量设备就完成计算。并且这些设备协议通常也都遵循事件驱动的方式。这也是边缘计算的可能。
三、Faas的实现 目前开源的Faas项目已经有许多，其中我较为熟悉的是fission、kubeless，可以参考适用于多媒体云处理的faas实现。
//TODO 补充Faas的实现思路
四、相关资料  Serverless/FaaS 的现状和未来 Report from workshop and panel on the Status of Serverless Computing and Function-as-a-Service(FaaS) in Industry and Research FaaS 如何在云 2.0 时代发挥优势，又将走向何方？ Serverless 应用开发指南  </description>
    </item>
    
    <item>
      <title>分布式异步回调模型的回调策略</title>
      <link>https://yesphet.github.io/posts/%E5%88%86%E5%B8%83%E5%BC%8F%E5%BC%82%E6%AD%A5%E5%9B%9E%E8%B0%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9B%9E%E8%B0%83%E7%AD%96%E7%95%A5/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0800</pubDate>
      
      <guid>https://yesphet.github.io/posts/%E5%88%86%E5%B8%83%E5%BC%8F%E5%BC%82%E6%AD%A5%E5%9B%9E%E8%B0%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9B%9E%E8%B0%83%E7%AD%96%E7%95%A5/</guid>
      <description>一、背景 客户端请求Web服务架构中，一般有同步阻塞模型和异步回调两种模型。对于服务端耗时较长，例如音视频转码等重操作的服务，异步回调模型相比同步模型有许多的优势:
 不会阻塞客户端的请求线程，可以提高客户端的线程利用率。 服务端根据自身的处理能力进行处理，保证服务端的稳定性。不会由于峰值请求造成服务端过载。  但是，在异步回调模型中，由于多了一次回调的链路，会带来更多的可用性问题。因此，本文主要讨论在异步回调模型中，如何制定有效的回调策略来保证回调链路的成功率，并提高整个异步服务的可用性。
二、异步回调模型 异步回调模型可以参考引用1中的描述，一般有以下两种细分模型：
2.1 Asynchronous Web Service Using a Single Request Queue 2.2 Asynchronous Web Service Using a Request and a Response Queue 其中2.2的模型虽然更复杂，但可以有效的提高服务端的资源使用率。避免由于回调阻塞导致处理能力的下降。同时可以增加一些判重策略，防止回调服务出现故障时，由于客户端重试导致服务端重复处理的资源浪费。
在分布式服务场景下，2.2 模型还可以细分为Inner Response Queue及outer Response Queue两种：
 Inner Response Queue为每个服务端app使用内存队列作为Response Queue，由内部的线程作为Callback Client。 Outer Response Queue为使用统一的消息队列中间件作为Response Queue，另外部署一套Callback Client服务来处理这些Response。  Outer模型相比Inner模型部署结构较为复杂，但与处理结构完全解耦，可以针对回调做更多策略，同时可以防止由于处理app宕机造成的Response丢失（不过由于callback client以及消息中间件策略的问题，仍然会存在response丢失的风险）。
以上各个模型各有优劣，应该根据业务场景选择合适的模型。
三、回调策略 对于异步回调模型，callback service一般由业务方提供，无法对可用性做保证。因此callback client必须要制定一些策略尽量的应对callback service失败的情况。尤其是在一个callback client对应多个callback service的场景下，需要尽量防止由于某些service的问题，影响回调其他service的情况。
Callback Service 失败场景： 1. client与service之间出现网络波动，甚至中断。 callback client请求service时，响应域名解析失败或者client请求超时。
快速重试策略可以解决网络波动问题。轮询重试策略可以解决短时间中断问题。
2. service超时。 callback service处理回调超时。这种情况除去网络问题，一般是由于service负载过高，或者设计存在问题导致。这种场景如果过度重试一般会造成service雪崩。解决的方案是周知业务方，由业务方对callback service进行排查。</description>
    </item>
    
    <item>
      <title>适用于多媒体云处理的Faas实现</title>
      <link>https://yesphet.github.io/posts/%E9%80%82%E7%94%A8%E4%BA%8E%E5%A4%9A%E5%AA%92%E4%BD%93%E4%BA%91%E5%A4%84%E7%90%86%E7%9A%84faas%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Thu, 21 Jun 2018 15:17:43 +0800</pubDate>
      
      <guid>https://yesphet.github.io/posts/%E9%80%82%E7%94%A8%E4%BA%8E%E5%A4%9A%E5%AA%92%E4%BD%93%E4%BA%91%E5%A4%84%E7%90%86%E7%9A%84faas%E5%AE%9E%E7%8E%B0/</guid>
      <description>一、背景 云处理平台，是公司基于私有云存储打造的一个针对图片、视频进行格式转换、裁剪、转码以及许多自定义操作的平台，类似于七牛云的多媒体API、阿里云的智能媒体管理。全量支持了公司所有图片及视频的处理。
最早的时候，云处理平台和云存储是一起诞生的，是作为云存储的一个子项目，在同一套系统中实现。虽然部署的时候是按照上传、下载、处理分集群进行部署，但是随着功能性的提升，项目代码越来越复杂，部署的成本也逐渐升高。因此我们进行了第一次重写，将其独立了开来，但仍旧是使用Java+Tomcat的方式部署在物理机上。
2018年初，随着业务的发展，云处理平台遇到了许多瓶颈和痛点。其中最为明显的是：
 新增一个处理接口的成本过高，这里的成本主要指上线及部署成本，需要由运维同学发布到每一台处理集群的物理机（那时我们已经实现了新增一个处理接口只需要编写处理脚本即可，不需要改动到Java代码，类似于Faas中的 『开发人员只需要关注功能代码的开发』 ）。 集群难以继续细分。由于面对公司很多产品，每个产品使用的处理接口不尽相同，且各接口的复杂度差异也很大（例如视频转码接口耗时是十秒级别，而大部分图片处理接口耗时是百毫秒级别）。由于都是重CPU操作，因此当集群负载高时，各接口可能互相影响，导致响应时间变慢。 云处理平台只支持同步请求，异步处理的方式是通过部署一个队列处理机来消费队列，解析队列中的消息再通过Http请求云处理平台，得到处理结果后，再回调业务方。整个流程变长，增加了部署管理成本，也更加的不可控。 集群扩容缩容麻烦。当业务上要进行推广时，只能由运维手动加入机器到集群，部署云处理服务，缩容也是一样。虽然后来有了自助上线平台，但成本依然较高，部署耗时长，风险大。 机器使用率无法很好的利用。由于是物理机部署，扩容缩容成本高，因此为了服务的可用性，必须要保证集群有较多的冗余，防止峰值流量将集群打垮的情况。 &amp;hellip;  适时正好Faas非常流行，公司也正在推进容器化，因此我们便又进行了新的一次重构，以Faas的思想实现了目前的云处理平台。
二、前期调研 前期调研了 fission 和 kubeless 两个开源的faas项目。两者都是基于k8s实现。调研时，fission的实现更为完善一些，所以侧重看了一些fission的源码。
fission的实现是通过trigger层来接受event，再通过http请求router层映射到对应的function，获取或启动对应的pod来处理这个event。其中trigger层类似于http服务器，或者队列处理机，router层担任的角色类似于API网关。而function层则是真正执行任务的地方，其事先由poolManager根据不同environment（go/binary/python&amp;hellip;）启动pod，pod对router层暴露http端口。当router层收到一个event时，根据url映射到对应function，再找到可以处理该event的pod（如果没有则立即创建，冷启动时间为100ms，pod启动时是没有function代码的，需要到etcd中拉取function代码，因此也限制了代码的大小必须在1MB内），最后将event通过http请求到可以执行的pod中。
那么fission适合我们云处理平台的场景吗？其实并不契合的。主要有几点：
 代码大小的限制，由于云处理除了一些简单的脚本外，有的脚本还需要依赖一些素材等资源文件，这些都属于function的一部分，这些的体积基本都会&amp;gt;1MB。 function pod统一暴露的都是http服务，虽然trigger层有支持mqtrigger（类似于队列处理机），但一些视频转码的请求一般需要几十秒甚至分钟级别，通过保持http连接进行通信不够可靠。 function代码是pod启动的时候临时拉取的。同第一点，云处理的function体积较大，启动时再拉取一个是影响冷启动时间，一个是会对带宽造成额外压力。  不是fission不好，而是因为云处理本身的场景就不在fission的考虑范围内，fission更倾向于支持的是小型、快速、生命周期短的function，例如一些解耦的非常干净的http处理函数。这也是AWS Lambda中的主要业务形态。
通过fission，我们也大概明白了faas的实现方式，参考了许多设计的方式，并实现了云处理平台的Faas系统 &amp;ndash; Trident 和 Poseidon。
其中Trident是函数管理系统，类似于fission中的controller。用于管理函数，发布服务，弹性扩缩容（HPA）。
Poseidon 则是运行时，包含trigger，environment，function等运行模块。本文也会着重介绍Poseidon
三、Poseidon 3.1 基本结构 Poseidon 的基本结构如下：
1. Trigger trigger层作为最上层，直接与前端交互，负责前端消息的接受和响应。这边的前端消息是指例如Http Request、消息队列中的消息。前端响应指例如Http Response、回调等。 目前Trigger层有4种实现：
 Http Server Kaproxy Consumer Kafka Consumer Command Line Interface  2. Protocol protocol层负责对前端消息的内容进行解析，构造成标准输入。将标准输入传递给Controller层进行下一步处理，之后解析Controller层响应的标准输出，构造成Trigger层的响应并返回给Trigger层。
Protocol interface { // 解析Http Request，并传递给Controller层处理。将处理结果转换为response返回 HttpParse(ctx context.</description>
    </item>
    
    <item>
      <title>Tomcat7压测(2)</title>
      <link>https://yesphet.github.io/posts/tomcat7%E5%8E%8B%E6%B5%8B2/</link>
      <pubDate>Tue, 14 Feb 2017 00:00:00 +0800</pubDate>
      
      <guid>https://yesphet.github.io/posts/tomcat7%E5%8E%8B%E6%B5%8B2/</guid>
      <description>目的  本篇将请求的耗时调大，主要为了观察线程池及等待队列都满了的情况下，Tomcat的表现及反应。  初步压测描述 先描述下一开始进行这部分压测遇到的问题。首先根据Tomcat7官方文档描述：
 Each incoming request requires a thread for the duration of that request. If more simultaneous requests are received than can be handled by the currently available request processing threads, additional threads will be created up to the configured maximum (the value of the maxThreads attribute). If still more simultaneous requests are received, they are stacked up inside the server socket created by the Connector, up to the configured maximum (the value of the acceptCount attribute).</description>
    </item>
    
    <item>
      <title>Tomcat7压测(1)</title>
      <link>https://yesphet.github.io/posts/tomcat7%E5%8E%8B%E6%B5%8B1/</link>
      <pubDate>Wed, 08 Feb 2017 00:00:00 +0800</pubDate>
      
      <guid>https://yesphet.github.io/posts/tomcat7%E5%8E%8B%E6%B5%8B1/</guid>
      <description>Tomcat7压测（一） 目的   目的主要有几点：
 了解空跑的TomcatQPS能达到多少 测试在Tomcat线程池，等待队列，最大连接数满了的情况下Tomcat的表现 通过压测暴露一些问题及对这些问题的解决 提升性能测试的姿势，算是一种锻炼吧    应该会分几个篇幅来分析，因为压测场景及数据比较多。
  本篇主要是最基础的测试，后续会对压测中发现的问题做一些解决
  压测 场景           Tomcat配置 maxThreads=&amp;quot;300&amp;quot;minSpareThreads=&amp;quot;100&amp;quot;connectionTimeout=&amp;quot;8000&amp;quot;enableLookups=&amp;quot;false&amp;quot;acceptCount=&amp;quot;100&amp;quot;acceptorThreadCount=&amp;quot;1&amp;rdquo;    环境 Docker容器内    服务描述 当收到一个请求后，将处理该请求的线程sleep20ms,然后返回response。因为该服务本身几乎不占用任何系统资源，所以在CPU，Mem，IO上是不会产生瓶颈的（实际压测证实），且因为该场景下控制了每个请求的耗时，QPS不高，所以在网络IO上也不会产生瓶颈。因此会影响QPS的只有Tomcat线程池的参数和Tomcat的性能    服务耗时 20ms     数据(整理后)    序号 API TotalTime Requests per second Time per request Tomcat线程池线程数 描述     1 ab -c100 -n500000 -k http://172.</description>
    </item>
    
  </channel>
</rss>